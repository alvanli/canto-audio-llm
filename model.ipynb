{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92af4efa-28fd-4bef-af42-227e2bebe288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|███████████████████████████████████████████████████| 4/4 [06:03<00:00, 90.78s/it]\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████| 4/4 [00:10<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2Model, AutoTokenizer, Qwen2ForCausalLM\n",
    "import torch\n",
    "\n",
    "llm_path = \"Qwen/Qwen2.5-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "llm_decoder = Qwen2ForCausalLM.from_pretrained(llm_path)\n",
    "\n",
    "sentence = \"Hi How are you? My name is Al\"\n",
    "tokens = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", max_length=100, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = llm_decoder(**tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ae2d02f-98a5-40ff-aaec-d3c9f1d0ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████| 4/4 [00:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Hi How are you? My name is Al\n",
      "Actual token length: 9\n",
      "Padding token ID: 151643\n",
      "\n",
      "Input tokens:\n",
      "['Hi', 'ĠHow', 'Ġare', 'Ġyou', '?', 'ĠMy', 'Ġname', 'Ġis', 'ĠAl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████| 4/4 [00:06<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Hi How are you? My name is Al\n",
      "Actual token length: 9\n",
      "Padding token ID: 151643\n",
      "\n",
      "Input tokens:\n",
      "['Hi', 'ĠHow', 'Ġare', 'Ġyou', '?', 'ĠMy', 'Ġname', 'Ġis', 'ĠAl']\n",
      "\n",
      "Early predictions (positions 0-8):\n",
      "\n",
      "Position 0:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.000032\n",
      "Top 3 predictions:\n",
      "  ' I': 0.200899\n",
      "  ',': 0.121025\n",
      "  ' i': 0.086963\n",
      "\n",
      "Position 1:\n",
      "Predicted token: ' can'\n",
      "Pad token probability: 0.000000\n",
      "Top 3 predictions:\n",
      "  ' can': 0.354732\n",
      "  ' to': 0.200555\n",
      "  ' do': 0.155959\n",
      "\n",
      "Position 2:\n",
      "Predicted token: ' you'\n",
      "Pad token probability: 0.000000\n",
      "Top 3 predictions:\n",
      "  ' you': 0.937362\n",
      "  ' u': 0.015357\n",
      "  ' You': 0.013532\n",
      "\n",
      "Position 3:\n",
      "Predicted token: ' doing'\n",
      "Pad token probability: 0.000565\n",
      "Top 3 predictions:\n",
      "  ' doing': 0.119657\n",
      "  '?\n",
      "': 0.113948\n",
      "  '?': 0.109641\n",
      "\n",
      "Position 4:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.001637\n",
      "Top 3 predictions:\n",
      "  ' I': 0.275233\n",
      "  ' My': 0.040477\n",
      "  ' Can': 0.037609\n",
      "\n",
      "Position 5:\n",
      "Predicted token: ' name'\n",
      "Pad token probability: 0.000003\n",
      "Top 3 predictions:\n",
      "  ' name': 0.782143\n",
      "  ' Name': 0.015966\n",
      "  ' question': 0.012304\n",
      "\n",
      "Position 6:\n",
      "Predicted token: ' is'\n",
      "Pad token probability: 0.000001\n",
      "Top 3 predictions:\n",
      "  ' is': 0.974264\n",
      "  ''s': 0.010346\n",
      "  '’s': 0.003482\n",
      "\n",
      "Position 7:\n",
      "Predicted token: ' John'\n",
      "Pad token probability: 0.000025\n",
      "Top 3 predictions:\n",
      "  ' John': 0.024239\n",
      "  ' David': 0.008868\n",
      "  ' Mary': 0.008666\n",
      "\n",
      "Position 8:\n",
      "Predicted token: 'ina'\n",
      "Pad token probability: 0.000060\n",
      "Top 3 predictions:\n",
      "  'ina': 0.088670\n",
      "  'aa': 0.057944\n",
      "  'vin': 0.042477\n",
      "\n",
      "Late predictions (positions 90-99):\n",
      "\n",
      "Position 90:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.011627\n",
      "Top 3 predictions:\n",
      "  ' I': 0.294071\n",
      "  ' My': 0.065615\n",
      "  ' To': 0.043103\n",
      "\n",
      "Position 91:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.023663\n",
      "Top 3 predictions:\n",
      "  ' I': 0.547420\n",
      "  ' Good': 0.041856\n",
      "  ' Nice': 0.031994\n",
      "\n",
      "Position 92:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.019600\n",
      "Top 3 predictions:\n",
      "  ' I': 0.488437\n",
      "  ' Hi': 0.036568\n",
      "  ' Hello': 0.034620\n",
      "\n",
      "Position 93:\n",
      "Predicted token: ' __'\n",
      "Pad token probability: 0.000376\n",
      "Top 3 predictions:\n",
      "  ' __': 0.105315\n",
      "  ' ______': 0.073079\n",
      "  ' is': 0.062906\n",
      "\n",
      "Position 94:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.002224\n",
      "Top 3 predictions:\n",
      "  ' I': 0.103694\n",
      "  ' Good': 0.082335\n",
      "  ' Nice': 0.057494\n",
      "\n",
      "Position 95:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.000884\n",
      "Top 3 predictions:\n",
      "  ' I': 0.087616\n",
      "  ',': 0.056766\n",
      "  ' Al': 0.048876\n",
      "\n",
      "Position 96:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.047217\n",
      "Top 3 predictions:\n",
      "  ' I': 0.187427\n",
      "  'I': 0.079453\n",
      "  '<|endoftext|>': 0.047217\n",
      "\n",
      "Position 97:\n",
      "Predicted token: ' My'\n",
      "Pad token probability: 0.008984\n",
      "Top 3 predictions:\n",
      "  ' My': 0.231192\n",
      "  ' I': 0.191794\n",
      "  ' And': 0.029845\n",
      "\n",
      "Position 98:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.019018\n",
      "Top 3 predictions:\n",
      "  ' I': 0.480545\n",
      "  ' Good': 0.032170\n",
      "  '\\': 0.021295\n",
      "\n",
      "Position 99:\n",
      "Predicted token: ' I'\n",
      "Pad token probability: 0.008336\n",
      "Top 3 predictions:\n",
      "  ' I': 0.514509\n",
      "  ' Hi': 0.034666\n",
      "  'I': 0.021814\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, Qwen2ForCausalLM\n",
    "import torch\n",
    "\n",
    "def analyze_predictions(sentence, start_pos, end_pos, llm_path=\"Qwen/Qwen2.5-7B\"):\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "    model = Qwen2ForCausalLM.from_pretrained(llm_path)\n",
    "    \n",
    "    # Tokenize input with padding\n",
    "    tokens = tokenizer(sentence, \n",
    "                      return_tensors=\"pt\", \n",
    "                      padding=\"max_length\", \n",
    "                      max_length=100, \n",
    "                      truncation=True)\n",
    "    \n",
    "    # Get the padding token ID\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # Get actual input tokens for comparison\n",
    "    input_ids = tokens['input_ids'][0].tolist()\n",
    "    actual_length = len(tokenizer.encode(sentence))\n",
    "    \n",
    "    print(f\"Original sentence: {sentence}\")\n",
    "    print(f\"Actual token length: {actual_length}\")\n",
    "    print(f\"Padding token ID: {pad_token_id}\")\n",
    "    print(\"\\nInput tokens:\")\n",
    "    print(tokenizer.convert_ids_to_tokens(input_ids[:actual_length]))\n",
    "    \n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    # Analyze predictions for specified range\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    for i in range(start_pos, end_pos):\n",
    "        logits = outputs.logits[0, i, :]\n",
    "        probs = torch.softmax(logits, dim=0)\n",
    "        \n",
    "        # Get top 3 predictions\n",
    "        top_probs, top_tokens = torch.topk(probs, 3)\n",
    "        \n",
    "        pred_token = torch.argmax(logits).item()\n",
    "        pad_prob = probs[pad_token_id].item()\n",
    "        \n",
    "        predictions.append({\n",
    "            'position': i,\n",
    "            'predicted_token': tokenizer.decode([pred_token]),\n",
    "            'predicted_id': pred_token,\n",
    "            'pad_probability': pad_prob,\n",
    "            'top_3': [(tokenizer.decode([t.item()]), p.item()) \n",
    "                     for t, p in zip(top_tokens, top_probs)]\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Analyze both ranges from your example\n",
    "sentence = \"Hi How are you? My name is Al\"\n",
    "early_preds = analyze_predictions(sentence, 0, 9)\n",
    "late_preds = analyze_predictions(sentence, 90, 100)\n",
    "\n",
    "print(\"\\nEarly predictions (positions 0-8):\")\n",
    "for pred in early_preds:\n",
    "    print(f\"\\nPosition {pred['position']}:\")\n",
    "    print(f\"Predicted token: '{pred['predicted_token']}'\")\n",
    "    print(f\"Pad token probability: {pred['pad_probability']:.6f}\")\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for token, prob in pred['top_3']:\n",
    "        print(f\"  '{token}': {prob:.6f}\")\n",
    "\n",
    "print(\"\\nLate predictions (positions 90-99):\")\n",
    "for pred in late_preds:\n",
    "    print(f\"\\nPosition {pred['position']}:\")\n",
    "    print(f\"Predicted token: '{pred['predicted_token']}'\")\n",
    "    print(f\"Pad token probability: {pred['pad_probability']:.6f}\")\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for token, prob in pred['top_3']:\n",
    "        print(f\"  '{token}': {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "581a0fb1-7ad2-4398-b37b-e4a537fc9004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bac053a8-44c8-47c3-b807-c0e7ddb3f7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 13048,   2585,    525,    498,     30,   3017,    829,    374,   1674,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f513396e-54f1-460f-9b6c-75319793418e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.argmax(output.logits[:,-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "676c4c14-a636-4467-a53e-16412f1990e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ina'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.argmax(output.logits[:,8,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d572fc42-da17-48e8-b3b3-545f74a15296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 152064])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31c9c709-bd7c-4c3e-8a16-0fe2d83a7034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'， are you? I name is John1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tokens = []\n",
    "for i in range(9):\n",
    "    a = torch.argmax(output.logits[:,i,:])\n",
    "    a = a.detach().cpu().numpy()\n",
    "    out_tokens.append(a)\n",
    "tokenizer.decode(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb5fb298-c35d-4148-8b8e-b082b5d61ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' .  is is., My\" '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tokens = []\n",
    "for i in range(90, 100):\n",
    "    a = torch.argmax(output.logits[:,i,:])\n",
    "    a = a.detach().cpu().numpy()\n",
    "    out_tokens.append(a)\n",
    "tokenizer.decode(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d687202-0e63-4d0f-9ccb-623c1c752750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
